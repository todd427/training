# ===== PyTorch (CUDA) =====
# Use the official CUDA 12.8 wheel index for GPU builds. If you want CPU-only, see the note below.
--index-url https://download.pytorch.org/whl/cu128
--extra-index-url https://pypi.org/simple

torch==2.8.0             # matches cu128 wheels; good on 4060 (Ada)
# torchvision / torchaudio not required for our training loop, omit to avoid version pin churn

# ===== Core HF stack =====
transformers>=4.44,<4.57
tokenizers>=0.20,<0.22
accelerate>=0.33,<0.36
datasets>=2.19,<3.0
huggingface-hub>=0.23,<1.0
safetensors>=0.4,<1.0

# LoRA / TRL
peft>=0.12,<0.19
trl>=0.9,<0.22
bitsandbytes>=0.44,<0.48      # QLoRA; if it gives you trouble on Py3.12, you can temporarily remove

# ===== Utils =====
einops>=0.7,<1.0
psutil>=5.9
tqdm>=4.66
regex>=2024.4
pydantic>=2.7,<3.0
PyYAML>=6.0,<7.0
requests>=2.31
packaging>=23.2
filelock>=3.12
numpy>=1.26
scipy>=1.11
pandas>=2.2,<3.0
pyarrow>=15
xxhash>=3.4

# ===== Extraction helpers (epub/gdoc) =====
beautifulsoup4>=4.12
ebooklib>=0.18
lxml>=4.9
python-docx>=1.1

# ===== Optional NLP helpers used in some scripts =====
nltk>=3.9
scikit-learn>=1.4


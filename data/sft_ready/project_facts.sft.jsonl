{"id": "f79863fde5aadcd7", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our current **base model target**?"}, {"role": "assistant", "content": "--; --; **Base model:**; Start with **GPT-OSS-20B** (the new open-weight release from OpenAI, ~21B params) as the foundation.; Fine-tune it with your blended dataset (writing, emails, answers, persona samples)."}]}
{"id": "7c540e85c372342a", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our **larger/pro tier model** plan?"}, {"role": "assistant", "content": "--; --; **Base model:**; Start with **GPT-OSS-20B** (the new open-weight release from OpenAI, ~21B params) as the foundation.; Fine-tune it with your blended dataset (writing, emails, answers, persona samples)."}]}
{"id": "3347b399581183ab", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what are the **key dataset sources** for SFT?"}, {"role": "assistant", "content": "--; --; **Base model:**; Start with **GPT-OSS-20B** (the new open-weight release from OpenAI, ~21B params) as the foundation.; Fine-tune it with your blended dataset (writing, emails, answers, persona samples)."}]}
{"id": "7d576b9c5a579de4", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is the **MVP roadmap**?"}, {"role": "assistant", "content": "--; --; **Base model:**; Start with **GPT-OSS-20B** (the new open-weight release from OpenAI, ~21B params) as the foundation.; Fine-tune it with your blended dataset (writing, emails, answers, persona samples)."}]}
{"id": "749cb3f560a81936", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our current **base model target**?"}, {"role": "assistant", "content": "--; [ ] **Download GPT-OSS-20B** weights.; Use Hugging Face or the official OpenAI OSS release. (~40–60 GB download).; Store in `~/models/gpt-oss-20b/` on **Lava** first.; [ ] **Install inference framework**:"}]}
{"id": "6d258a4194c2d9f8", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our **larger/pro tier model** plan?"}, {"role": "assistant", "content": "--; [ ] **Download GPT-OSS-20B** weights.; Use Hugging Face or the official OpenAI OSS release. (~40–60 GB download).; Store in `~/models/gpt-oss-20b/` on **Lava** first.; [ ] **Install inference framework**:"}]}
{"id": "0be19e6654e8c5cd", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, how are we **serving the model** (engine/config)?"}, {"role": "assistant", "content": "--; [ ] **Download GPT-OSS-20B** weights.; Use Hugging Face or the official OpenAI OSS release. (~40–60 GB download).; Store in `~/models/gpt-oss-20b/` on **Lava** first.; [ ] **Install inference framework**:"}]}
{"id": "03a562988bd06fff", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what are the **key dataset sources** for SFT?"}, {"role": "assistant", "content": "--; [ ] **Download GPT-OSS-20B** weights.; Use Hugging Face or the official OpenAI OSS release. (~40–60 GB download).; Store in `~/models/gpt-oss-20b/` on **Lava** first.; [ ] **Install inference framework**:"}]}
{"id": "04036da9b744515d", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, how are we doing **side-by-side/eval**?"}, {"role": "assistant", "content": "--; [ ] **Download GPT-OSS-20B** weights.; Use Hugging Face or the official OpenAI OSS release. (~40–60 GB download).; Store in `~/models/gpt-oss-20b/` on **Lava** first.; [ ] **Install inference framework**:"}]}
{"id": "cd8b881d7fb0e123", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is the **MVP roadmap**?"}, {"role": "assistant", "content": "--; [ ] **Download GPT-OSS-20B** weights.; Use Hugging Face or the official OpenAI OSS release. (~40–60 GB download).; Store in `~/models/gpt-oss-20b/` on **Lava** first.; [ ] **Install inference framework**:"}]}
{"id": "d57de56d4e2b588c", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our current **base model target**?"}, {"role": "assistant", "content": "--; --; --; --; **Core (GPU + CPU + RAM + Case):** ~€3,552"}]}
{"id": "654a1ff3c7a8beb7", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what are the **key dataset sources** for SFT?"}, {"role": "assistant", "content": "--; --; --; --; **Core (GPU + CPU + RAM + Case):** ~€3,552"}]}
{"id": "e8d86d4f1c9ce2b9", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our current **base model target**?"}, {"role": "assistant", "content": "--; **Training / fine-tuning LLMs**: 5090 gives you 32 GB VRAM → run GPT-OSS-20B at full context, experiment with LoRA/RAG locally, test training pipelines before cloud scaling.; **Reducing dependency on cloud**: RunPod, HuggingFace, or DO GPUs cost **€0.80–1.20/hr (L40S/A100)**. At ~€20/day, you hit **€600/mo** easily. In less than a year, you’d have spent more than the desktop’s cost in cloud…"}]}
{"id": "dc5434da4d8f7690", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our **larger/pro tier model** plan?"}, {"role": "assistant", "content": "--; **Training / fine-tuning LLMs**: 5090 gives you 32 GB VRAM → run GPT-OSS-20B at full context, experiment with LoRA/RAG locally, test training pipelines before cloud scaling.; **Reducing dependency on cloud**: RunPod, HuggingFace, or DO GPUs cost **€0.80–1.20/hr (L40S/A100)**. At ~€20/day, you hit **€600/mo** easily. In less than a year, you’d have spent more than the desktop’s cost in cloud…"}]}
{"id": "9f80a8d79bfba58a", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our current **base model target**?"}, {"role": "assistant", "content": "--; Runs GPT-OSS-20B (quantized) on Lava.; Persona layer: your tone + safety rules baked via system prompt and mini-dataset.; Simple UI: CLI → later a minimal desktop app.; Distribution: free demo (limited context/usage) + paid Lite license."}]}
{"id": "93bbf84a315951d0", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our **larger/pro tier model** plan?"}, {"role": "assistant", "content": "--; Runs GPT-OSS-20B (quantized) on Lava.; Persona layer: your tone + safety rules baked via system prompt and mini-dataset.; Simple UI: CLI → later a minimal desktop app.; Distribution: free demo (limited context/usage) + paid Lite license."}]}
{"id": "633a984b88466818", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what are the **key dataset sources** for SFT?"}, {"role": "assistant", "content": "--; Runs GPT-OSS-20B (quantized) on Lava.; Persona layer: your tone + safety rules baked via system prompt and mini-dataset.; Simple UI: CLI → later a minimal desktop app.; Distribution: free demo (limited context/usage) + paid Lite license."}]}
{"id": "9feac5c7ce1cefc4", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is the **MVP roadmap**?"}, {"role": "assistant", "content": "--; Runs GPT-OSS-20B (quantized) on Lava.; Persona layer: your tone + safety rules baked via system prompt and mini-dataset.; Simple UI: CLI → later a minimal desktop app.; Distribution: free demo (limited context/usage) + paid Lite license."}]}
{"id": "e45aea1fda0602cc", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what are our **safety/guardrail** choices?"}, {"role": "assistant", "content": "--; Runs GPT-OSS-20B (quantized) on Lava.; Persona layer: your tone + safety rules baked via system prompt and mini-dataset.; Simple UI: CLI → later a minimal desktop app.; Distribution: free demo (limited context/usage) + paid Lite license."}]}
{"id": "6340f643c07e27d2", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our current **base model target**?"}, {"role": "assistant", "content": "--; Finalize data pipeline (parse → clean → pack).; Run GPT-OSS-20B locally on Lava.; Build CLI demo with persona + system prompt.; Expand fine-tune dataset to ~100 entries."}]}
{"id": "3d0547509a1c74f0", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our **larger/pro tier model** plan?"}, {"role": "assistant", "content": "--; Finalize data pipeline (parse → clean → pack).; Run GPT-OSS-20B locally on Lava.; Build CLI demo with persona + system prompt.; Expand fine-tune dataset to ~100 entries."}]}
{"id": "d82394ccec2bdb63", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what are the **key dataset sources** for SFT?"}, {"role": "assistant", "content": "--; Finalize data pipeline (parse → clean → pack).; Run GPT-OSS-20B locally on Lava.; Build CLI demo with persona + system prompt.; Expand fine-tune dataset to ~100 entries."}]}
{"id": "0e99ff9523198e3c", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is the **MVP roadmap**?"}, {"role": "assistant", "content": "--; Finalize data pipeline (parse → clean → pack).; Run GPT-OSS-20B locally on Lava.; Build CLI demo with persona + system prompt.; Expand fine-tune dataset to ~100 entries."}]}
{"id": "3c6be3d3052beb8f", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our current **base model target**?"}, {"role": "assistant", "content": "--; --; AI assistants today are **cloud-dependent** (privacy risks, recurring costs).; Writers, students, and professionals need **always-available, private tools**.; Cloud GPU time is expensive (€0.80–1.20/hr) → unsustainable for constant use."}]}
{"id": "67d2bd20a236c233", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our **larger/pro tier model** plan?"}, {"role": "assistant", "content": "--; --; AI assistants today are **cloud-dependent** (privacy risks, recurring costs).; Writers, students, and professionals need **always-available, private tools**.; Cloud GPU time is expensive (€0.80–1.20/hr) → unsustainable for constant use."}]}
{"id": "d358596453ca7e8f", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what are the **key dataset sources** for SFT?"}, {"role": "assistant", "content": "--; --; AI assistants today are **cloud-dependent** (privacy risks, recurring costs).; Writers, students, and professionals need **always-available, private tools**.; Cloud GPU time is expensive (€0.80–1.20/hr) → unsustainable for constant use."}]}
{"id": "cd857d95f5004bb7", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is the **MVP roadmap**?"}, {"role": "assistant", "content": "--; --; AI assistants today are **cloud-dependent** (privacy risks, recurring costs).; Writers, students, and professionals need **always-available, private tools**.; Cloud GPU time is expensive (€0.80–1.20/hr) → unsustainable for constant use."}]}
{"id": "5857e7e8675e86ff", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what are our **safety/guardrail** choices?"}, {"role": "assistant", "content": "--; --; AI assistants today are **cloud-dependent** (privacy risks, recurring costs).; Writers, students, and professionals need **always-available, private tools**.; Cloud GPU time is expensive (€0.80–1.20/hr) → unsustainable for constant use."}]}
{"id": "f6c1ad9c4f91cefd", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our current **base model target**?"}, {"role": "assistant", "content": "--; --"}]}
{"id": "6a1926965645be57", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our **larger/pro tier model** plan?"}, {"role": "assistant", "content": "--; --"}]}
{"id": "1e1fde302e6eb035", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our **larger/pro tier model** plan?"}, {"role": "assistant", "content": "--; **USP**: “Your AI never leaves your machine.”; **Audience**: privacy-sensitive, power users, devs.; **Requirements**: high (16GB+ VRAM ideal).; **Pricing**: one-time license (cover cost of development)."}]}
{"id": "a2740bdfc2c1f0f5", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, how are we doing **side-by-side/eval**?"}, {"role": "assistant", "content": "--; **USP**: “Your AI never leaves your machine.”; **Audience**: privacy-sensitive, power users, devs.; **Requirements**: high (16GB+ VRAM ideal).; **Pricing**: one-time license (cover cost of development)."}]}
{"id": "8b327ea6c1d51297", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our **larger/pro tier model** plan?"}, {"role": "assistant", "content": "--; **OSS model** = the “brain.”; **Agentic layer** = orchestrator:; File search (local docs, notes, manuscripts).; Local APIs (calendar, email via IMAP, system commands)."}]}
{"id": "e26e9cc35c3fd89d", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is the **MVP roadmap**?"}, {"role": "assistant", "content": "--; **OSS model** = the “brain.”; **Agentic layer** = orchestrator:; File search (local docs, notes, manuscripts).; Local APIs (calendar, email via IMAP, system commands)."}]}
{"id": "957070617877b299", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our **larger/pro tier model** plan?"}, {"role": "assistant", "content": "--; Run **OSS model locally** (chat only).; Wrap it with a **tiny API** (OpenAI-compatible `/chat/completions`).; Deliver: “It runs on your box. It talks like Todd.”; --"}]}
{"id": "0525635e4709bec4", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is the **MVP roadmap**?"}, {"role": "assistant", "content": "--; Run **OSS model locally** (chat only).; Wrap it with a **tiny API** (OpenAI-compatible `/chat/completions`).; Deliver: “It runs on your box. It talks like Todd.”; --"}]}
{"id": "a971941bd3d276e8", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our current **base model target**?"}, {"role": "assistant", "content": "**Always-on uptime** (so users have a reliable endpoint).; **Enough GPU headroom** to serve multiple sessions concurrently.; **Networking** that’s stable (port mapping, reverse proxy, TLS).; It sits on your network → runs vLLM/llama.cpp.; NAS handles reverse proxy and SSL."}]}
{"id": "29c1c870a6d0877c", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is our **larger/pro tier model** plan?"}, {"role": "assistant", "content": "**Always-on uptime** (so users have a reliable endpoint).; **Enough GPU headroom** to serve multiple sessions concurrently.; **Networking** that’s stable (port mapping, reverse proxy, TLS).; It sits on your network → runs vLLM/llama.cpp.; NAS handles reverse proxy and SSL."}]}
{"id": "765217bde15ad1a9", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, how are we **serving the model** (engine/config)?"}, {"role": "assistant", "content": "**Always-on uptime** (so users have a reliable endpoint).; **Enough GPU headroom** to serve multiple sessions concurrently.; **Networking** that’s stable (port mapping, reverse proxy, TLS).; It sits on your network → runs vLLM/llama.cpp.; NAS handles reverse proxy and SSL."}]}
{"id": "a35ac69846122bea", "source": "project_facts", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "As of 2025-08-27, what is the **MVP roadmap**?"}, {"role": "assistant", "content": "**Always-on uptime** (so users have a reliable endpoint).; **Enough GPU headroom** to serve multiple sessions concurrently.; **Networking** that’s stable (port mapping, reverse proxy, TLS).; It sits on your network → runs vLLM/llama.cpp.; NAS handles reverse proxy and SSL."}]}
